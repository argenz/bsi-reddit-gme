{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5573f08",
   "metadata": {},
   "source": [
    "# Reddit & GME Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5cf00",
   "metadata": {},
   "source": [
    "### By Francesca Argenziano, Fatbard Luari and Marco Massobrio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46482e56",
   "metadata": {},
   "source": [
    "This is a extracurricular project conducted by the aforementioned team of students as part of the BSI Student Assocation. Francesca carried out the sections of Approach, Data Extraction, Sentiment and Conclusion. Marco carried out the section on Research. Fatbard carried out the section on Data Exploration, which is omitted because he left the team for an internship at the end and his work was never commited."
    "[Edit Jan 2022: This was my (Francesca) first attempt at NLP, learning anything by myself and from scratch. With my current knowledge, I realize that some of the following is wrong. Word Embeddings should be mapped to a lower dimentional spaces with methods such as Word2Vec or GloVe. The following merely provides a VxD matrix of word count occurences in each document. Furthermore, numerous best practices that should be carried out were ignored due lack of knowledge and time constraints , such as proper model selection and evaluation.]\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ab62c",
   "metadata": {},
   "source": [
    "## Introduction & Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05732c2",
   "metadata": {},
   "source": [
    "The \"short-squeeze\": a compelling episode that took place in early 2021 involving the Reddit's subreddit r/wallstreetbets, powerful hedge funds, and previously declining stocks, amongst which special attention was given to the video game retailer Game Stop (GME). What happened exactly? In short (pardon the pun), retail investors got organised on r/wallstreetbets to mass-purchase GME and other stocks, which funds like Melvin Capital had heavily shorted. This led to an increase in share price, causing lossess to the hedge funds, which in turn bought more shares to compensate for the losses, hence driving the share price up even more. GME stock price skyrocketed from 19 USD to 347 USD in the month of January alone. \n",
    "\n",
    "We are interested in identifying this phenomenon by looking at the sentiment of submissions (or posts) on r/wallstreetbets, and in asserting the extent of its relationship with the share price. Theoretically, the stock price is a represesntation of sentiment about a specific asset or company: if many people appreciate your products or believe in your vision, i.e. have positive sentiment about your company, they are likely to buy and hold your stock hence leading to an increase its price. Analysing the submissions on the subreddit is another way of obtaining such latent sentiment. Therefore, our expectations are that the sentiment of the posts and the price should follow similar trends. Given that its a concise phenomenon involving the interaction of various parties (investors, trading platforms, stock exchanges, news outlets...) we are also interested in looking at how exogenous events influenced the stock price.\n",
    "\n",
    "Hence our research questions are:  \n",
    "1. *How does the sentiment trend on r/wallstreetbets compare to GME's stock price?* \n",
    "2. *How were the sentiment and the stock price influenced by major exogenous events?*\n",
    "\n",
    "Our approach consists in four main phases: Data Extraction, Data Exploration, Research, and Sentiment Analysis. We begin by obtianing the data necessary for the analysis from the relevant APIs; then we explore the data: we look at the daily frequency of submissions, most used words, emojis and specific r/wallstreetbets \"lingo\". Further, we look into the event itself gather information to answer out second research question. Then we proceed with analysing the sentiment using three main approaches on a varying spectrum of model customisability, from the most off-the-shelf with textBlob, to a Vader model updatd with specific \"lingo\", to a custom classifier trained with word embedding techniques. Finally, we gather our findings, evaluate them and conclude our research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b73bc70",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e10811",
   "metadata": {},
   "source": [
    "First, we searched for an efficient data extraction method. To do that, we found the yahoo finance Python library, whit which it is possible to extract historical data about stock prices. \n",
    "If you are interested in exploring more about this library, check this website: https://pypi.org/project/yfinance/\n",
    "What we did is to extract stock price data from the 20th of March to the 20th of May with a time interval of one hour.  By doing so, we included in our time horizon the main events that may have affected GME stock price but at the same time we keep a fairly restricted amount of data, which could have taken a considerable amount of time to be processed otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "def get_stockdata(name, start, end, interval):\n",
    "    GME = yf.Ticker(name)\n",
    "    stock_data = GME.history(start=start, end=end, interval=interval, prepost= True)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b94db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"GME\"\n",
    "start = \"2020-03-20\"\n",
    "end = \"2020-05-20\"\n",
    "interval = \"1h\"\n",
    "\n",
    "stock_data = get_stockdata(name, start, end, interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3886252",
   "metadata": {},
   "source": [
    "Here we define the methods to pull the Reddit submissions data from the Pushift API. The offical Reddit API is called PRAW, but we used Pushshift because it \"was designed and created by the /r/datasets mod team to help provide enhanced functionality and search capabilities for searching Reddit comments and submissions.\", for example, querying a specific time-range is not availale in PRAW, whilst Pushift provides this needed capability. \n",
    "\n",
    "Given the large amount of data, we throttled the requests: splitting the time interval queried in smaller intervals, which query results were saved to different CSV files. This is because, with one single request, the CSV file would become too large and crash before saving, hence leaving us with nothing but a dead kernel o_o. Throught throttling, we were able to divide the workload in separate files, and if one time interval ended up containing too many submissions, we still had the data for the previous intervals saved and ready to go.\n",
    "\n",
    "In order obtain data about the sentiment of GME, we looked at the submissions contianing the word \"GME\" in the title. We requested the following information regarding each submission: the url, author, title, created_utc (date and time of submission), id, score (upvotes - downvotes), and the upvote_ratio (upvotes/all votes). We did not fetch the actual text of the submission because of the size of the body of text, and decided to proceed with NLP techniques solely on the titles. The fields we actually ended up using are only the author, title, and created_utc fields. A direction for future improvements is to perform a deeper analysis on the submission by looking at the scores and upward ratios and look for sentiment trends in these subgroups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "import logging as log\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "api = PushshiftAPI()\n",
    "start_time = dt.datetime(2020, 12, 20) \n",
    "end_time = dt.datetime(2021,  2, 19)\n",
    "\n",
    "# CSV \n",
    "# splits the time frame (%start_time and %end_time) in blocks of %time_delta days \n",
    "# Creates a separate .csv file for each block\n",
    "# Each .csv files contains the Reddit submissions containing the word \"GME\" in the title\n",
    "def throttle_requests_tocsv(start_time, end_time, time_delta):\n",
    "    tmp_etime = start_time + time_delta\n",
    "\n",
    "    while tmp_etime < end_time:\n",
    "        submissions = api.search_submissions(\n",
    "                            after=int(start_time.timestamp()),\n",
    "                            before=int(tmp_etime.timestamp()),\n",
    "                            subreddit='wallstreetbets',\n",
    "                            filter=['url', 'author', 'title',\n",
    "                                    'created_utc', 'id', 'score', 'upvote_ratio'],\n",
    "                            )\n",
    "\n",
    "        with open(f'wsb_{start_time}_{tmp_etime}.csv', 'w') as f: \n",
    "            sub_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "            sub_writer.writerow(['id', 'created_utc', 'author', 'title', 'url', 'score', 'upvote_ratio'])\n",
    "\n",
    "            for sub in submissions: \n",
    "                wrds = sub.title.split()\n",
    "                if \"GME\" not in wrds: continue \n",
    "                else: \n",
    "                    log.warning(f\"Sub ID:{sub.id}, Sub created_utc: {sub.created_utc}, Sub title: {sub.title[:10]}, Sub score:{sub.score}\")\n",
    "                    sub_writer.writerow([sub.id, sub.created_utc, sub.author, sub.title, sub.url, sub.score, sub.upvote_ratio])           \n",
    "\n",
    "        start_time = tmp_etime\n",
    "        tmp_etime = start_time + time_delta\n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafca76",
   "metadata": {},
   "source": [
    "Once the function throttle_requests_tocsv is defined, we can run once the commands below to download all the csv files, and to create the dataframe that we'll be using for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0be30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "throttle_requests_tocsv(start_time, end_time, dt.timedelta(10)) #10 day interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4899b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- -- -- -- Parsing from CSV to pandas.DataFrame -- -- -- -- #\n",
    "df_dict = {}\n",
    "for filename in os.listdir(\"./files\"):\n",
    "    df_dict[filename] = pd.read_csv(f\"./files/{filename}\")\n",
    "\n",
    "sub_df = pd.concat([df for df in df_dict.values()]).reset_index(drop=True)\n",
    "\n",
    "# convert utc to datetime\n",
    "sub_df[\"created_utc\"] = sub_df[\"created_utc\"].apply(lambda x: dt.datetime.fromtimestamp(int(x)).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "sub_df.rename(columns={'created_utc':'created'}, inplace=True)\n",
    "\n",
    "sub_df.to_csv(\"sub_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194c237",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a87ab1",
   "metadata": {},
   "source": [
    "We searched for the most important events occurred during the stock rally that could have influenced the trading price. Below you can see a summary of what we found and the intra-day gain or loss.\n",
    "At a first look, we can surely see some correlation between the events occurred in the \"real world\" and shares market movements. Even so, we can see how most of the gain was made on the aftermarket of Jan. 26 opening the next day at more than +300%. Surely E. Musk and C. Palihapitiya tweets made some influence on that, but it is difficult to identify the extent of this influence as the positions of some large investors such as the Melvin fund had already been covered its positions throughout the day due to the massive volume of transactions triggered by WSB/subreddit.\n",
    "Then, from Jan. 28 many brokers halted the possibility of buying GME shares, while it was still possible to sell them. This situation lasted for more than a week, as the shares became available to the public again on Feb. 5, giving the stock an 18% increase.\n",
    "Keeping in mind all those information and being conscious that many factors have influenced the stock price we will proceed with our analysis that should be considered as a single point of view of an higher order complex phenomena. \n",
    "\n",
    "If you want to know more in details all the events occurred, check those websites: \n",
    "- https://en.m.wikipedia.org/wiki/GameStop_short_squeeze\n",
    "- https://bsic.it/the-game-theory-of-gamestop/\n",
    "- https://www.vgen.it/blog/davide-vs-golia-il-caso-gamestop-rispolvera-lo-scontro-sul-mercato-azionario/\n",
    "- https://abcnews.go.com/amp/Business/gamestop-timeline-closer-saga-upended-wall-street/story?id=75617315"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76679ea",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b8b98",
   "metadata": {},
   "source": [
    "There are many tools available for semtiment analysis. On a spectrum between off-the-shelf and custom, off-the-shelf tools are fast, easy and accessible libraries of pre-trained models that semalessly provide a measure of polarity (positive or negative sentiment) on a given text input. On the other hand, custom models are possbly more accurate and a better fit to the specific problem you're trying to solve, but they take more time and require a deeper understanding of the underlying techniques. Finally, there are also many solutions in between: off-the-shelf that offer some customisation, advanced techniques that can be applied quickly. We begin by looking at TexBlob, the easiest and quickest, then we move to Vader, which offers some customisation, and the we explore other sentiment techniques using wor embeddings. \n",
    "\n",
    "Before applying any technique, we preprocess our dataset in order to strip it from characters and unuseful text. We lowercase all the text in submissions, remove stop words, and lemmatize the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc54a1c",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25eb3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sub_df = pd.read_csv(\"sub_df.csv\")\n",
    "sent_df = sub_df.loc[:,(\"created\", \"author\", \"title\")]\n",
    "sent_df.loc[:,\"created\"] = pd.to_datetime(sub_df.loc[:,'created']).dt.floor('d') \n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c40534",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = pd.read_csv(\"gme_stock_data.csv\")\n",
    "stock_df = stock_df.loc[:, ~stock_df.columns.str.contains('^Unnamed')]\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f5beb",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the nltk download script in the next cell is broken, \n",
    "# run this script and double-click on \"all packages\" to download the necssary data from nltk. \n",
    "# you can do this once only, so you can ignore this cell if you re-run this notebook. \n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports necessary, ignore if you ever run it previously\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b82c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#part-of-speech tagging\n",
    "def get_wordnet_pos(text):\n",
    "  for tok, pos in nltk.pos_tag(word_tokenize(text)):\n",
    "    if pos[0].lower() =='j':\n",
    "      return wordnet.ADJ\n",
    "    elif pos[0].lower() =='v':\n",
    "      return wordnet.VERB\n",
    "    elif pos[0].lower() =='n':\n",
    "      return wordnet.NOUN\n",
    "    elif pos[0].lower() =='r':\n",
    "      return wordnet.ADV\n",
    "    else:\n",
    "      return wordnet.NOUN   #if anything else, just leave as noun\n",
    "\n",
    "#lemmatization\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def tagandlemma(title): \n",
    "  words = word_tokenize(title)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word, pos=f\"{get_wordnet_pos(word)}\") for word in words]\n",
    "  return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee2d4c",
   "metadata": {},
   "source": [
    "Here we below apply the preprocessing functions, since our dataset contains around 76K submissions, it might take a while to run, so be patient :) or upload the already preprocessed csv in the subsequent cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sent_df.loc[:,\"ptitle\"] = sent_df.loc[:,\"title\"].str.lower().str.replace(r'([^a-zA-Z\\s])|(^https?:\\/\\/.*[\\r\\n]*)', ' ')\n",
    "sent_df.loc[:,\"ptitle\"] = sent_df.loc[:,\"ptitle\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "sent_df.loc[:,\"ptitle\"] = sent_df.loc[:,\"ptitle\"].apply(lambda x: tagandlemma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload already preprocessed csv titles\n",
    "sent_df.loc[:,\"ptitle\"] = pd.read_csv(\"preproc_titles.csv\").loc[:,\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6cb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78dec5",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7fd569",
   "metadata": {},
   "source": [
    "We apply textBlob sentiment on the processed title column \"ptitle\", which returns a number between -1 for negative, and 1 for porisitve polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "#apply textBlob sentiment \n",
    "sent_df.loc[:,\"polarity_textBlob\"] = sent_df.loc[:,\"ptitle\"].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34a389",
   "metadata": {},
   "source": [
    "In order to plot the by sentiment by day, we create a new dataframe with two columns: \"created\" which hold the dates, and \"polarity_textBlob\" that takes the mean of the sentiment on that day. We also scale the sentiment values by dividing by the standard deviation in order to plot them together the stock price data. There are NaNs for stock price data on the days where the stock market is closed, so it's normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ab166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#organise sentiment by date and standarside\n",
    "daily_sent_df = sent_df.loc[:,(\"created\", \"polarity_textBlob\")].groupby([\"created\"], as_index=False).mean()\n",
    "daily_sent_df.loc[:,\"z_polarity_textBlob\"] = daily_sent_df.loc[:,\"polarity_textBlob\"]/daily_sent_df.loc[:,\"polarity_textBlob\"].std(axis=0)\n",
    "daily_sent_df.loc[:,\"created\"] = pd.to_datetime(daily_sent_df.loc[:,\"created\"])\n",
    "\n",
    "#standardize stock data as well\n",
    "stock_df.loc[:,\"z_open\"] = stock_df.loc[:,\"Open\"]/stock_df.std(axis=0).iloc[1]\n",
    "stock_df.loc[:,\"z_close\"] = stock_df.loc[:,\"Close\"]/stock_df.std(axis=0).iloc[1]\n",
    "stock_df.loc[:,\"Date\"] = pd.to_datetime(stock_df.loc[:,\"Date\"])\n",
    "\n",
    "# merge for plotting\n",
    "merged = daily_sent_df.merge(stock_df.rename(columns={'Date': 'created'}).loc[:,[\"created\",\"z_open\",\"z_close\"]], how='outer')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "mS = merged.loc[:,[\"created\",\"z_close\",\"z_polarity_textBlob\"]].melt('created', var_name='Legend',  value_name='vals')\n",
    "sns.lineplot(ax=ax, data=mS, x=\"created\", y=\"vals\", hue='Legend', palette=\"flare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d584b77",
   "metadata": {},
   "source": [
    "### Vader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c6a92",
   "metadata": {},
   "source": [
    "Because textBlob uses a \"Bag-of-Words\" approach based on a standard vocabulary, it performs relatively poorly. Vader is similar to textBlob, but it allows us to modify the vocabulary to include the specific lexicon used in our /wallstreetbets subreddit. So we use the data exploration analysis done previously to find the 100 most used words in the subreddit, and add them to our Vader Sentiment lexicon, and then we extract a new (hopefully improved) sentiment. \n",
    "\n",
    "We saved the most used word in the file \"lexicon.py\" as a dictionary, where to each word we manually assign a sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd77451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import file from directory\n",
    "#import sys  \n",
    "#sys.path.insert(0, '/Users/FCRA/Desktop/ALL/BSI/bsi-reddit-gme/pyfiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexicon import new_words_updated\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "#update the lexicon with the new words\n",
    "vader.lexicon.update(new_words_updated)\n",
    "\n",
    "#apply Vader sentiment \n",
    "sent_df.loc[:,\"polarity_vader\"] = sent_df.loc[:,\"ptitle\"].apply(lambda x: vader.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2aab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f8244",
   "metadata": {},
   "source": [
    "Similarly to before, we create a dataframe of sentiment by day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060832a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#organise sentiment by date and standarside\n",
    "daily_sent_df_vader = sent_df.loc[:,[\"created\", \"polarity_vader\"]].groupby([\"created\"], as_index=False).mean()\n",
    "daily_sent_df_vader.loc[:,\"z_polarity_vader\"] = daily_sent_df_vader.loc[:,\"polarity_vader\"]/daily_sent_df_vader.loc[:,\"polarity_vader\"].std(axis=0)\n",
    "daily_sent_df_vader.loc[:,\"created\"] = pd.to_datetime(daily_sent_df_vader.loc[:,\"created\"])\n",
    "\n",
    "# merge for plotting\n",
    "merged = merged.merge(daily_sent_df_vader.loc[:,[\"created\",\"z_polarity_vader\", \"polarity_vader\"]], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4307a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "mS = merged.loc[:,[\"created\",\"z_close\",\"z_polarity_textBlob\", \"z_polarity_vader\"]].melt('created', var_name='Legend',  value_name='vals')\n",
    "sns.lineplot(ax=ax, data=mS, x=\"created\", y=\"vals\", hue='Legend', palette=\"flare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4ae8b",
   "metadata": {},
   "source": [
    "We observe a more similar pattern between stock price and sentiment when we use Vader rather than textBlob, however our goal is to discover wether there is a pattern, not to assume there is one. Therefore, as we refine out sentiment analysis methods, we will be able to better assess the existence of this pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82902f2c",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ba328",
   "metadata": {},
   "source": [
    "The textBlob and Vader \"Bag-of-Words\" approach base the sentence sentiment solely on a superposition of the polarity of each individual word, without taking into account what words can mean in context. Therefore we turn to Word Wmbeddings to improve our analysis and attempt to obtain a more fine-grained masure of sentiment.\n",
    "\n",
    "Hence, we are going to use HansingVectorizer to map the each word in a word vector space, where similar words are close to eachother and dissimilar ones are not. We chose HashingVectorizer because of the memory efficiency. Then we use the Sentiment140 Dataset created by Alec Go, Richa Bhayani, and Lei Huang to train our Support Vector Classifier. The dataser consists of 1.6M tweets about various topics including Obama, Kindle, Windows 10... each with reliable sentiment labels. Once we trained the SVC, we obtian predictions on the sentiment of our Reddit posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a0718",
   "metadata": {},
   "source": [
    "### Preprocessing of Sentiment140 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb62c4b",
   "metadata": {},
   "source": [
    "Here is the code for how we preprocessed the training and testing sets. This takes a while, hence when running this notebook we advise skipping to the cell where we upload the already-preprocessed files below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data preproc\n",
    "rdf = pd.read_csv('reduced1600000KTrainingData.csv')\n",
    "rdf = rdf.loc[:, ~rdf.columns.str.contains('^Unnamed')]\n",
    "rdf.columns = [\"polarity\", \"ptitle\"]\n",
    "rdf[\"ptitle\"] = rdf[\"ptitle\"].str.lower().str.replace(r'([^a-zA-Z\\s])|(^https?:\\/\\/.*[\\r\\n]*)', '') \n",
    "rdf[\"ptitle\"] = rdf[\"ptitle\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "rdf[\"ptitle\"] = rdf[\"ptitle\"].apply(lambda x: tagandlemma(x))\n",
    "rdf.to_csv(\"preprocessed_training_final.csv\")\n",
    "\n",
    "#testing data preproc\n",
    "test = pd.read_csv('reducedTestingData.csv')\n",
    "test = test[[\"polarity\", \"text\"]]\n",
    "test.columns = [\"polarity\", \"ptitle\"]\n",
    "test[\"ptitle\"] = test[\"ptitle\"].str.lower().str.replace(r'([^a-zA-Z\\s])|(^https?:\\/\\/.*[\\r\\n]*)', '') \n",
    "test[\"ptitle\"] = test[\"ptitle\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "test[\"ptitle\"] = test[\"ptitle\"].apply(lambda x: tagandlemma(x))\n",
    "test.to_csv(\"preprocessed_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32ccec",
   "metadata": {},
   "source": [
    "Here we read the already preprocessed files. Furthermore, since the dataset is organised with all the negative posts at the beginning an the positive at the end, we randomise the rows in order to have randomised appearance of positive and negative comments, in order to avoid bias when triaining the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "rdf = pd.read_csv('preprocessed_training_final.csv')\n",
    "rdf = rdf.loc[:, ~rdf.columns.str.contains('^Unnamed')]\n",
    "rdf.columns = [\"polarity\", \"ptitle\"]\n",
    "\n",
    "#randomise rows\n",
    "rdf = rdf.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "rdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "test = pd.read_csv('preprocessed_test_final.csv')\n",
    "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cb312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting features and labels\n",
    "train = rdf.dropna(axis=0)\n",
    "train_x = train[\"ptitle\"] \n",
    "train_y = train[\"polarity\"]   \n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035a5fe",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b0c74",
   "metadata": {},
   "source": [
    "Given the large amount of data, we train in batches to avoid running out of RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer #,TfidfTransformer\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = HashingVectorizer()\n",
    "#tfTran = TfidfTransformer()\n",
    "#sklearn.feature_extraction.text.TfidfTransformerÂ¶\n",
    "\n",
    "clf = SGDClassifier(loss=\"modified_huber\") \n",
    "\n",
    "batch_size = 5000\n",
    "start = 0\n",
    "epochs = int(len(train_x.index)/batch_size)\n",
    "\n",
    "for i in range(epochs): \n",
    "  train_x_batch = train_x[start:(start+batch_size)]\n",
    "  train_y_batch = train_y[start:(start+batch_size)]\n",
    "  \n",
    "  vectors_x = vectorizer.fit_transform(train_x_batch)\n",
    "  clf.partial_fit(vectors_x, train_y_batch, classes=np.unique(train_y_batch))\n",
    "  start = start+batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b96fe",
   "metadata": {},
   "source": [
    "Now that the SDG is trained, we test the model with the testing dataset provided by Sentiment140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd646f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pn = test.loc[test[\"polarity\"]!=2].reset_index(drop=True) #remove neutral tweets since we had none in our training set\n",
    "predict = pd.DataFrame(test_pn.loc[:,[\"polarity\", \"ptitle\"]])\n",
    "\n",
    "test_x = test_pn.loc[:,\"ptitle\"]\n",
    "test_y = test_pn.loc[:,\"polarity\"]\n",
    "\n",
    "test_vectors_x = vectorizer.fit_transform(test_x.values.astype(str))\n",
    "predict.loc[:,\"pred\"] = clf.predict(test_vectors_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40780c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how many predictions were correct #DELETE, you can do with functions below\n",
    "count = 0\n",
    "for i in range(len(predict.index)):\n",
    "  if predict.loc[i,\"polarity\"]==predict.loc[i,\"pred\"]: \n",
    "    count=count+1\n",
    "right_pred = count/len(predict.index)\n",
    "right_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46202a",
   "metadata": {},
   "source": [
    "We obtain an accuracy of around 79%, which we consider good enough to proceed and predict the sentiment of our /wallstreetbets submission titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9512e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "y_pred = predict.loc[:,\"pred\"]\n",
    "y_true = test_pn.loc[:,\"polarity\"]\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430db3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b01d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2a535",
   "metadata": {},
   "source": [
    "### Prediction of WSB Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gme = sent_df.loc[:,\"ptitle\"]\n",
    "\n",
    "test_vectors_x = vectorizer.fit_transform(test_gme.values.astype(str))\n",
    "sent_df.loc[:,\"polarity_WEM\"] = clf.predict(test_vectors_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720de90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to specific polarity\n",
    "def convert_into_range(sent):\n",
    "  if sent==4: \n",
    "    return 1\n",
    "  else:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.loc[:,\"polarity_WEM\"] = sent_df.loc[:,\"polarity_WEM\"].apply(convert_into_range)\n",
    "sent_df.loc[:,\"polarity_WEM\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sent_df_WEM = sent_df.loc[:,(\"created\", \"polarity_WEM\")].groupby([\"created\"], as_index=False).sum()\n",
    "daily_sent_df_WEM.loc[:,\"z_polarity_WEM\"] = daily_sent_df_WEM.loc[:,\"polarity_WEM\"]/daily_sent_df_WEM.loc[:,\"polarity_WEM\"].std(axis=0)\n",
    "daily_sent_df_WEM.loc[:,\"created\"] = pd.to_datetime(daily_sent_df_WEM.loc[:,\"created\"])\n",
    "daily_sent_df_WEM\n",
    "\n",
    "#merge for plotting\n",
    "merged = merged.merge(daily_sent_df_WEM.loc[:,[\"created\",\"z_polarity_WEM\", \"polarity_WEM\"]], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "mS = merged.loc[:,[\"created\",\"z_close\",\"z_polarity_WEM\"]].melt('created', var_name='Legend',  value_name='vals')\n",
    "sns.lineplot(ax=ax, data=mS, x=\"created\", y=\"vals\", hue='Legend', palette=\"flare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot them all together \n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "mS = merged.loc[:,[\"created\",\"z_close\",\"z_polarity_textBlob\", \"z_polarity_vader\",\"z_polarity_WEM\"]].melt('created', var_name='Legend',  value_name='vals')\n",
    "sns.lineplot(ax=ax, data=mS, x=\"created\", y=\"vals\", hue='Legend', palette=\"flare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e8d91",
   "metadata": {},
   "source": [
    "# Conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610b8b2",
   "metadata": {},
   "source": [
    "The approach we used to conduct our analysis, while statistically rigorous and valid, has some areas of improvement and limitations.\n",
    "First, the correlation between submission sentiment and share price performance is not necessarily causally related. moreover, many events parallel to WSB's euphoria acted on the share price. For example, the block to the possibility of buying a stock was followed by a drop in the market. Also, when Elon Musk and Chamath Palihapitiya had a huge increase in the share price. The extent to which these events affected the market is difficult to estimate, as they occurred at a time of extreme volatility. The tweets of the entrepreneurs occurred on the day when the biggest rise in the share price was already being recorded, while the restrictions on the purchase of shares occurred a few days after the peak. Since the effect of these events on the market cannot be eliminated, it is difficult to understand how much our model can actually predict the trend of the action, and how much it is actually a model that can predict only this one event.\n",
    "The events mentioned above are only two of the countless aspects that influence markets, since markets are the result of investors' expectations and a number of other events, both exogenous and endogenous.\n",
    "Finally, the time horizon that we have considered is limited and our analysis could be able to explain mainly the events occourred in the time we considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80bd995",
   "metadata": {},
   "source": []
  }
  ],
 "metadata": {
  "interpreter": {
   "hash": "48827d035531ab3070869194bf051c268755a4eb926f5e1ed6208fcb889593ad"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
